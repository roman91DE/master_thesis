# TODO

NGRAM size
Pretraining pop size größer machen
lynch - program synthesis and variational autoencoders Paper - parameter für pretraining anschauen
convergence als neue trainingsmethode anstelle von ES
1. einfluss auf generalisierbarkeit durch pt?
2. benefit für die suche?
selektion rausnehmen
lev diversity over generations - pt vs reg?
pre-training: few shot learning

# Why use Pre-Training?

- Sampling Error bei kleinen Populationsgrößen wird mitgelernt - Overfitting der DAE-LSTM
- Premature Convergence
- Autoencoder wird in jeder Generation from Scratch erlernt - Sehr großes Rechenaufwand


# Which kind of Pre-Training

- Re-Using - Über Generationen weiter trainieren?
- 


Kim, 2014
- EDA in GP - Re-Using/Pre-Training Ansätze 


