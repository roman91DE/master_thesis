# TODO

NGRAM size
Pretraining pop size größer machen
lynch - program synthesis and variational autoencoders Paper - parameter für pretraining anschauen
convergence als neue trainingsmethode anstelle von ES
1. einfluss auf generalisierbarkeit durch pt?
2. benefit für die suche?
selektion rausnehmen
lev diversity over generations - pt vs reg?
pre-training: few shot learning

# Why use Pre-Training?

- Sampling Error bei kleinen Populationsgrößen wird mitgelernt - Overfitting der DAE-LSTM
- Premature Convergence
- Autoencoder wird in jeder Generation from Scratch erlernt - Sehr großes Rechenaufwand


# Which kind of Pre-Training

- Re-Using - Über Generationen weiter trainieren?
- 


Kim, 2014
- EDA in GP - Re-Using/Pre-Training Ansätze 


# To Do

* Conclusion: Beschreibe Trade-Off fuer PT. Dae-gp verliert die Fähigkeot Dae-lstms sich dynamisch
an die Komplexität der Population anzupassen. Auf der anderen Seite reduzieet sixh der Aufwand an Epochen, die trainiert werden müssen

* Andere Initialisierungsmethode fuer PT Population: Grow Trees

* Laufzeit Analyse!

