% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{float}
\usepackage[numbers]{natbib}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Pre-Trained Denoising Autoencoders Long Short-Term Memory Networks as probabilistic Models for Estimation of Distribution Genetic Programming

Master Thesis - M.Sc. Business Education

--------------------------------------------------------

Student: Roman Höhn

Date of Birth: 1991-04-14

Place of Birth: Wiesbaden, Hesse

Student ID: 2712497

Supervisor: David Wittenberg

--------------------------------------------------------

Master Thesis

FB 03: Chair of Business Administration and Computer Science

Johannes Gutenberg University Mainz}
\author{}
\date{\vspace{-2.5em}Date of Submission: 2023-01-27}

\begin{document}
\maketitle

\thispagestyle{empty} \newpage{}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\setcounter{page}{1}
\tableofcontents
\thispagestyle{empty}
\newpage
\listoftables
\listoffigures
\thispagestyle{empty}
\newpage

\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\ldots{}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Denoising Autoencoder Genetic Programming (DAE-GP) is a novel variation of an genetic programming based Estimation of Distribution Algorithm (EDA-GP) that uses a denoising autoencoders long short-term memory network (DAE-LSTMs) as a probabilistic model to sample new candidate solutions {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}.

DAE-LSTMs are artificial neural networks that can be trained in an unsupervised learning environment to minimize a reconstruction error for encoding input data into a compressed representation and subsequently decoding the compressed representation back to the input dimension.
In DAE-GP, DAE-LSTMs are trained with a subset of high-fitness solutions selected from a parent population with the aim to capture their promising qualities.
The resulting model is then used to sample new offspring solutions by propagating partially mutated solutions from the parent population through the DAE-LSTMS {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}.
In previous work DAE-GP has been shown to outperform GP for both a generalized version of the royal tree problem {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]} as well as for a real-world symbolic regression problem {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]}.

The DAE-GP algorithm first described by {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]} trains a DAE-LSTMs for each generation \(g\) of the search from scratch.
{[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]} and {[}\protect\hyperlink{ref-daegp_explore_exploit}{16}{]} suggests the incorporation of a pre-training strategy into the evolutionary search as a possible way of improving the overall performance of the DAE-GP algorithm.
The key idea is to pre-train an initial DAE-LSTM on a large population of candidate solutions and to use the pre-trained parameters of this initial model in each generation of the search as starting parameters for the current generation DAE-LSTM.
This thesis studies the influence of using pre-trained DAE-LSTMs in DAE-GP for symbolic regression, especially looking at the influence on overall quality of solutions found by DAE-GP and effects on runtime.
The aim of this study is to answer the question if a pre-training strategy can be used to improve DAE-GP and to study both positive and negative effects on overall performance.

\ldots{}

\hypertarget{theoretical-foundations}{%
\section{Theoretical Foundations}\label{theoretical-foundations}}

\emph{This section describes the relevant concepts that are necessary for the understanding and classification of DAE-GP as well as the concept of pre-training in artificial neural networks}

\hypertarget{denoising-autoencoder-genetic-programming}{%
\subsection{Denoising Autoencoder Genetic Programming}\label{denoising-autoencoder-genetic-programming}}

DAE-GP is an EDA-GP algorithm that uses DAE-LSTM networks as a probabilistic model to sample new offspring solutions {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}.

\hypertarget{evolutionary-computation}{%
\subsubsection{Evolutionary Computation}\label{evolutionary-computation}}

As a variant of GP, DAE-GP can be classified as a meta-heuristic that belongs to the field of evolutionary computation (EC).
EC based meta-heuristics are optimization methods that simulate the process of Darwinian evolution to search for high quality solutions by applying selection and variation operators to a population of candidate solutions.
Examples of EC include genetic algorithms (GA), evolutionary strategies (ES) and GP.
In EC, the quality of a solution is commonly measured as fitness and the time steps of the search are called generations.
Another important concept in EC is the distinction between genotypes and phenotypes of solutions, the genotype contains the information that is necessary to construct the phenotype, the outer appearance of a particular solution on which we measure the overall quality of solutions.
The representation of a solution is therefore defined by the mapping of genotypes to phenotypes {[}\protect\hyperlink{ref-design_of_modern_heuristics}{12}{]}.
Genetic operators, such as mutation or recombination are usually applied to the genotype of solutions.

\hypertarget{genetic-programming}{%
\subsubsection{Genetic Programming}\label{genetic-programming}}

GP follows the same basic evolutionary principle of EC but searches for more general, hierarchical computer programs of dynamically varying size and shape {[}\protect\hyperlink{ref-Koza1993GeneticP}{8}{]}.
The computer programs that are at the center of the evolutionary search in GP are commonly represented by tree structures at the level of their phenotype {[}\protect\hyperlink{ref-design_of_modern_heuristics}{12}{]}.
Since GP searches for high fitness computer programs that produce a desired output for some input, it can be applied to various different problem domains such as symbolic regression, automatic programming, or evolving game-playing strategies {[}\protect\hyperlink{ref-Koza1993GeneticP}{8}{]}.
An important quality of GP is the ability to search for solutions of variable length and structure.
GP is an especially useful meta heuristic for problems where no a priori knowledge about the final form of good solutions is available.
GPs ability to optimize solutions for their structure as well as for their parameters led to it being one of the most prevalent methods used for symbolic regression {[}\protect\hyperlink{ref-10.1007ux2f978-3-540-24621-3_22}{10}{]}.

\hypertarget{estimation-of-distribution-algorithms}{%
\subsubsection{Estimation of Distribution Algorithms}\label{estimation-of-distribution-algorithms}}

The aim of Estimation of distribution algorithms (EDA) is to replace the standard variation operators used in GA by building probabilistic models that can capture complex dependencies between different decision variables of an optimization problem {[}\protect\hyperlink{ref-design_of_modern_heuristics}{12}{]}.
EDAs use this probabilistic model to sample new offspring solutions inside an evolutionary search to replace crossover and/or mutation operators.

\hypertarget{denoising-autoencoders}{%
\subsubsection{Denoising Autoencoders}\label{denoising-autoencoders}}

One possible way of model building in EDA proposed by {[}\protect\hyperlink{ref-harmless_overfitting_eda}{11}{]} is to use denoising autoencoders (DAEs) as generative models to capture complex probability distributions of decision variables.
DAE, a variation of the autoencoders (AE), are a widely used type of neural networks in the domain of unsupervized machine learning that maps \(n\) input variables to \(n\) output variables using a hidden representation.

AE were introduced by {[}\protect\hyperlink{ref-ae_orig}{6}{]} to compress high-dimensional data into lower-dimensions. An AE consists of two different subunits:

\begin{itemize}
\tightlist
\item
  Encoder \(g(x)\): Encode input data to a smaller cental layer \(h\)
\item
  Decoder \(d(h)\): Decode and output the the encoded data back to its original dimension
\end{itemize}

The AE is trained to reduce the reconstruction error between input and output data, after the training procedure is finished the network is able to reduce the dimensionality of input data to a compressed representation{[}\protect\hyperlink{ref-ae_orig}{6}{]}.

DAE was first introduced by {[}\protect\hyperlink{ref-dae_orig2008}{14}{]} as an improved AE with the ability to learn new representations of data that is especially robust to partially corrupted input data. DAE modifies the AE by using partially corrupted input data for the AE and training it to reconstruct the uncorrupted, original version of the input data.

Since the hidden representation of DAE captures the dependency structure of the input data it can therefore also be used to generate new solutions in the context of GAs {[}\protect\hyperlink{ref-harmless_overfitting_eda}{11}{]}.

DAE-GP builds upon the concept of using DAEs in EDAs described by {[}\protect\hyperlink{ref-harmless_overfitting_eda}{11}{]} and transfers the concept to the domain of GP.
The mutation and crossover operators of standard GP are replaced by sampling new solutions from a probabilistic model that is build by training a DAEs long short-term memory (LSTM) network on a subset of high fitness solutions from the current population {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}.

LSTMs are a variant of artificial neural networks first introduced by {[}\protect\hyperlink{ref-lstm_orig}{7}{]} that can store learned information over en extended time period while avoiding the problem of vanishing and/or exploding gradients. Since DAE-GP encodes candidate solutions as linear strings in prefix notation, the DAE in DAE-GP uses LSTMs for both encoding and decoding where the total amount of time steps \(T\) is equal to sum of the length of the input solution and the output solution {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}.

\hypertarget{pre-training}{%
\subsection{Pre-Training}\label{pre-training}}

Pre-Training describes the concept of initially training an artificial neural network on a large dataset before using it to solve a more specific task.
It is a commonly used strategy in deep architectures that has been shown to improve both the optimization process itself as well as the generalization behavior if compared to the standard approach of using randomly initialized parameters {[}\protect\hyperlink{ref-pmlr-v5-erhan09a}{3}{]}.

The strategy of pre-training artificial neural networks is based on the idea of transfer learning that aims at retaining previously learned knowledge from one task to re-use it for another task {[}\protect\hyperlink{ref-HAN2021225}{5}{]} {[}\protect\hyperlink{ref-survey_transfer_learning}{9}{]}.
Transfer learning traditionally follows a two phase approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pre-Training Phase: Capture knowledge from source task
\item
  Fine-Tuning Phase: Transfer knowledge to the target task
\end{enumerate}

where source task and target task are usually similar but may differ in their feature space and the distribution of training data {[}\protect\hyperlink{ref-survey_transfer_learning}{9}{]}.

Some of the main motivations for using pre-training is the ability to reduce the need for large amounts of training data which can often be unavailable or too expensive to collect as well as an improved performance by either reducing the computational effort that is necessary to train a model to solve a specific task or by improving the models generalisation ability.

\ldots describe different pre-training strategies, e.g.~few shot, perpetual, re-using\ldots{}

-- describe unsupervized vs supervized pre-training\ldots{}

\hypertarget{pre-training-in-dae-gp}{%
\section{Pre-Training in DAE-GP}\label{pre-training-in-dae-gp}}

The pre-training strategy used in this thesis is applied to the DAE-LSTM model \(M_g\) that are used in each DAE-GP generation \(g\) for \(g\in{1,...,g_{max}}\).
Instead of initializing and optimizing each model from scratch as done in previous work (e.g. {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]} {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]}), a separate DAE-LSTM network \(\hat{M}\) will be trained on a large population of randomly initialized solutions.\\
The trainable parameters \(\theta_{\hat{M}}\) obtained after finishing the training procedure of \(\hat{M}\) are then used as the starting parameters for each following DAE-LSTM model \(M_g\) for \(g\in{1,...,g_{max}}\).

The motivation for incorporating pre-training into DAE-GP is based on the following suspected mechanisms of improvement:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Improve overall performance of DAE-GP by improving either run time and/or solution quality
\item
  Reduce the need for large population sizes to avoid sampling error
\item
  Improve model robustness and generalization ability
\end{enumerate}

The probably most obvious motivation for using a pre-training strategy in DAE-GP is based on the fact, that initializing \(M_g\) with pre-trained weights \(\theta_{\hat{M}}\) is likely to reduce the amount of training epochs that are necessary until the point of training error convergence is reached.

Early research on pre-training for DAE by {[}\protect\hyperlink{ref-pmlr-v5-erhan09a}{3}{]} comes to the conclusion that besides adding robustness to models the strategy also results in improved generalization and better performing models. The authors describe that unsupervised pre-training behaves like a regulizer on DAE networks, the mean test error and it's variance are reduced for DAE networks that are initialized from pre-trained weights if compared to the same architectures that use random initialization.

Another important finding by {[}\protect\hyperlink{ref-pmlr-v5-erhan09a}{3}{]} is that the positive effect of pre-training is dependent on both the depth of the network as well as the size of layers - while increasingly larger networks benefit increasingly more from pre-training the final performance of small architectures tends to be worse with pre-training than with randomized weight initialization.
The authors find evidence that pre-training DAE is especially usefull for optimizing the parameters of the lower layers of the network.

Another motivation for introducing pre-training into DAE-GP is the prevalence of sampling error for small GP populations sizes.
{[}\protect\hyperlink{ref-sampling_err_gp}{13}{]} finds that sampling error, non-systematic errors that are caused by observing only a small subset of the statistical population, is a severe problem in the domain of GP since the initial population may not be a representative sample of all possible solutions.
{[}\protect\hyperlink{ref-sampling_err_gp}{13}{]} also introduces a method for calculating optimal population sizes to minimize the presence of sampling error.
Since DAE-LSTM of DAE-GP learns the properties of it's training population and reuses the acquired knowledge in the sampling procedure, using the parameters obtained from training \(\hat{M}\) on a sufficiently large training population might reduce the need for large population sizes in the following generations of the search if \(\hat{M}\) already implicitly captured the properties of a representative sample of solutions.
If this mechanism can be applied successfully it would benefit the performance of DAE-GP by increasing the population diversity (resulting in better solution quality) as well as by reducing the need for large population sizes which require computational resources.

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

The DAE-GP algorithm, first described by {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}, is summarized as a flowchart in figure \ref{fig:daegp-flowchart}.
After setting the generation counter \(g\) to \(0\) and creating the initial population of solutions \(P_0\), DAE-GP enters the main loop that checks if a termination criteria is satisfied (i.e.~maximum number of generations or fitness evaluations is reached).
For each generation \(g\) the fitness of all new individuals in \(P_g\) is evaluated. During model building, DAE-GP selects a subset \(X_g\) of the current population that is used as training data for the DAE-LSTM model \(M_g\) which is trained to learn the properties of \(X_g\).
The trained model \(M_g\) is then used for model sampling, partially mutated solutions from \(P_G\) are propagated through \(M_g\) to create a new population \(P_{g+1}\).
This process is repeated until the termination criteria is met and DAE-GP returns the highest fitness solutions.

\begin{figure}[c]

{\centering \includegraphics[width=0.6\linewidth]{./img/flowcharts/dae-gp} 

}

\caption{Regular DAE-GP Flowchart}\label{fig:daegp-flowchart}
\end{figure}

The pre-training strategy implemented for all experiments in this thesis is visualized as another flowchart in figure \ref{fig:pt-daegp-flowchart}.
The main difference to regular DAE-GP is the inclusion of an initial pre-training phase where a separate population \(\hat{P}\) is first initialized and then randomly split by half into \(\hat{P}_{train}\) and \(\hat{P}_{test}\).
A DAE-LSTM model \(\hat{M}\) is then trained to learn the properties of \(\hat{P}_{train}\) using an early stopping training mode where we stop training as soon as the validation error for \(\hat{P}_{test}\) converges.
After the training for \(\hat{M}\) is stopped, the current state of \(\hat{M}\) is frozen and the optimized trainable parameters \(\theta_{\hat{M}}\) are saved before terminating the pre-training phase to start the DAE-GP phase.
During the DAE-GP phase, the only difference to the traditional DAE-GP algorithm described in figure \ref{fig:daegp-flowchart} is during model building: We initialize \(M_g\) with \(\theta_{\hat{M}}\) before training it to learn the properties of \(X_g\).

\begin{figure}[c]

{\centering \includegraphics[width=0.65\linewidth]{./img/flowcharts/pt-dae-gp} 

}

\caption{Pre-Trained DAE-GP Flowchart}\label{fig:pt-daegp-flowchart}
\end{figure}

One difficulty in the implementation of a pre-training strategy into DAE-GP has been the detemination of the number of hidden neurons in side the DAE-LSTMs hidden layers. The original description of DAE-GP {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]} uses a strategy where the number of hidden neurons for each hidden layer is dynamically set per generation to the maximum individual size inside the current population.
For a pre-training implementation, this strategy can not be easily adapted since it leads to a changing number of neurons at each generation resulting in different dimensions of the DAE-LSTM. To allow the sharing of the state of the pre-trained parameters \(\theta_{\hat{M}}\) from the pre-trained model \(\hat{M}\) to each \(M_g\) for \(g\in{1,...,g_{max}}\) I set the number of hidden neurons per hidden layer for all pre-trained runs statically to the maximum individual size inside the pre-training population \(\hat{P}_{train}\)

If not stated otherwise, all experiments in this thesis use the hyperparameters listed in table \ref{tab:airfoil-fullRun-2hl-maxIndSize-params}.

\begin{table}

\caption{\label{tab:airfoil-fullRun-2hl-maxIndSize-params}DAE-GP - Hyperparameter}
\centering
\begin{tabular}[t]{l|l}
\hline
\textbf{Hyperparameter} & \textbf{Value}\\
\hline
Population\_Size & 500\\
\hline
Generations & 30\\
\hline
Fitness\_Metric & RMSE\\
\hline
Training\_Mode & Convergence\\
\hline
Sampling\_Steps & 2\\
\hline
Selection\_Operator & Binary Tournament Selection\\
\hline
Corruption\_Operator & Levenshtein Edit\\
\hline
Denoising\_Edit\_Probability(training/sampling) & 0.05/0.95\\
\hline
Function\_Set & \{+ , - , * , aq\}\\
\hline
Ephemeral\_Constants & [-5,..,5]\\
\hline
Pre-Training\_Population\_Size & 10000\\
\hline
Pre-Training\_Train/Test\_Split & 50\%\\
\hline
Pre-Training\_Training\_Mode & Early Stopping\\
\hline
\end{tabular}
\end{table}

The framework that was used to conduct all experiments of this thesis was provided by the supervisor of this thesis David Wittenberg and uses the python programming language in conjunction with the baseline libraries \texttt{Keras} {[}\protect\hyperlink{ref-chollet2015keras}{1}{]} for deep neural networks and \texttt{deap} {[}\protect\hyperlink{ref-DEAP_JMLR2012}{4}{]} for evolutionary computation.

\hypertarget{benchmark-problem}{%
\subsection{Benchmark Problem}\label{benchmark-problem}}

To test pre-training in DAE-GP this thesis focuses on the domain of real-world symbolic regression problems. Symbolic Regression problems have been one of the first GP applications {[}\protect\hyperlink{ref-Koza1993GeneticP}{8}{]} and are an actively studied and highly relevant research area. The goal in symbolic regression is to find a mathematical model for a given set of data points {[}\protect\hyperlink{ref-10.1007ux2f978-3-540-24621-3_22}{10}{]}, in real-world symbolic regression these data points are sourced from real-world observations which in contrast to synthetic symbolic regression problem are more likely to contain random noise and bias. Another important challenge in solving real-world symbolic regression problems is the ability for a given model to generalize, we want the final model to show high accuracy in predicting outcomes for previously unseen cases.

The main experiments conducted in this thesis uses the NASA Airfoil Self-Noise Data Set which is part of the UCI machine learning repository {[}\protect\hyperlink{ref-machine_learning_repo}{2}{]}. The dataset consists of 5 input variables and 1 output variable that are listed in table \ref{tab:airfoil-dataset-description}.

\begin{table}[!h]

\caption{\label{tab:airfoil-dataset-description}Airfoil - Dataset Description}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
\textbf{Type} & \textbf{Name} & \textbf{Description} & \textbf{Unit}\\
\hline
input & x1 & Frequency & Hertz\\
\hline
input & x2 & Angle of attack & Degree\\
\hline
input & x3 & Chord length & meters\\
\hline
input & x4 & Free-stream velocity & meters/second\\
\hline
input & x5 & Suction side displacement thickness & meters\\
\hline
output & y & Scaled sound pressure level & decibels\\
\hline
\end{tabular}
\end{table}

The objective of the airfoil problem is to find a function that accurately predicts the output variable \(y\) by taking in a subset of the input variables \(x1,x2,x3,x4,x5\). The function set used by all DAE-GP variations for the airfoil problem is summarized in table 2, the terminal set consists of the 5 input variables \(x1,x2,x3,x4,x5\) and ephemeral random integers in the range of \([{}-5,..,5]\) {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]}.

Besides studying the performance for the airfoil dataset, additional real world symbolic regressions experiments used the datasets{[}\protect\hyperlink{ref-machine_learning_repo}{2}{]} described in table \ref{tab:full-run-realWorldSymReg-problems}.

\begin{table}[!h]

\caption{\label{tab:full-run-realWorldSymReg-problems}Real World Symbolic Regression Benchmark Problems}
\centering
\begin{tabular}[t]{l|r|r}
\hline
\textbf{Problem} & \textbf{Observations} & \textbf{Features}\\
\hline
Airfoil & 1503 & 5\\
\hline
Boston\_Housing & 506 & 13\\
\hline
Energy\_Cooling & 768 & 8\\
\hline
Concrete & 1030 & 8\\
\hline
\end{tabular}
\end{table}

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{effect-on-generalisation}{%
\subsection{Effect on Generalisation}\label{effect-on-generalisation}}

To gather a deeper understanding about the effect of pre-training DAE-LSTM in DAE-GP a series of experiments was conducted using the airfoil dataset for symbolic regression while using and different parameter configurations for the number of hidden layers as well as the number of hidden neurons per hidden layer.

To study generalization behavior this series of experiments is conducted using DAE-GP with only a single generation until the search terminates. I disregard the fitness of solutions found and focus soley on the reconstruction error that is produced during the training of each DAE-LSTM. The reconstruction error is measured for two separate populations, a training population \(P_{train}\) that is used to train our DAE-LSTM as well as a hold-out validation population \(P_{test}\). For the pre-trained DAE-GP two additional, separate populations \(\hat{P}_{train}\) and \(\hat{P}_{test}\) are used exclusively for pre-training.

DAE-GP is tested in two different configurations:

\begin{itemize}
\tightlist
\item
  Variable number of hidden neurons (50, 100, 200) with a single hidden layer
\item
  Fixed number of hidden neurons (100) per hidden layer with variable number of hidden layers (1, 2, 3)
\end{itemize}

For each configuration I tested traditional DAE-GP as well as a pre-trained DAE-GP resulting in 12 total sub experiments that were each based on 10 individual runs (total number of runs=\(120\)). To avoid creating biased results through the presence of sampling error inside the pre-training population (see {[}\protect\hyperlink{ref-sampling_err_gp}{13}{]}), the population size for the pre-training phase is chosen very high with the size of \(\hat{P}\) =20000 where 50\% of \(\hat{P}\) is used for the training population \(\hat{P}_{train}\) and the remaining 50\% are used for the hold-out validation population \(\hat{P}_{test}\). The training of DAE-LSTM uses a fixed number of 1000 epochs until termination and uses Adam optimization for gradient descent. The reason for using a high amount of 1000 fixed training epochs is to deliberately force the DAE-LSTM to overfit to the training data.

In general I expect that with a growing number of model parameters (either by adding more hidden layers or more hidden neurons per layer) the DAE-LSTM will be more prone to overfit to the training population \(P_{train}\) resulting in a small reconstruction error for the training population and a large one for the validation population \(P_{test}\). The reason for this effect is that a larger network trained over an extended period of time (without the use of strategies like early stopping), has much more potential to learn noise from the training dataset than a smaller network, which is more likely to result in worsening performance for previously unseen cases {[}\protect\hyperlink{ref-weigend1994overfitting}{15}{]}.

Based on the review of {[}\protect\hyperlink{ref-pmlr-v5-erhan09a}{3}{]} I also expect that pre-training will have an insignificant or even negative influence on small DAE-LSTM instances while improving the networks generalization ability with growing size.

\ldots{}

\begin{figure}[c]

{\centering \includegraphics[width=0.9\linewidth]{./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_neurons_3plots} 

}

\caption{First Generation Median Training Error for variable number of hidden Neurons}\label{fig:first-gen-airfoil-byNeurons}
\end{figure}

\ldots{}

\begin{figure}[c]

{\centering \includegraphics[width=0.9\linewidth]{./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_layers_3plots} 

}

\caption{First Generation Median Training Error for variable number of hidden Layers}\label{fig:first-gen-airfoil-byLayers}
\end{figure}

\hypertarget{effect-on-search-performance}{%
\subsection{Effect on search performance}\label{effect-on-search-performance}}

After closely examining the effect of pre-training on DAE-GPs generalization behaviour another series of experiments is conducted to study how pre-training influences the overall search behaviour of DAE-GP. The airfoil problem was selected as a first benchmark problem to test pre-trained DAE-GP against traditional DAE-GP.

Regarding the hidden neurons, the dimension of each hidden layer, I used two different strategies: Regular DAE-GP uses the same strategy as described by earlier work (see {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]} or {[}\protect\hyperlink{ref-dae-gp_2020_rtree}{18}{]}), the number of hidden neurons is dynamically set at each generation to the maximum length of all solutions inside the current training population. For pre-trained DAE-GP another strategy had to be used, I used a fixed number of hidden neurons for each run that is set equal to the maximum length of all individuals that are present in the initial pre-training population. I decided to take this aproach instead of using a fixed number of hidden neurons for the fact that it simplifies the adjustment of the parameter for later experiments on different problems. It should nonetheless be mentioned that this strategy does give pre-training on average more hidden neurons per layer resulting in a more complex network (since the pre-training population is much larger than the regular population, the chances for larger individuals inside it are also higher).

\hypertarget{airfoil-dateset---2-hidden-layers}{%
\subsubsection{Airfoil Dateset - 2 hidden Layers}\label{airfoil-dateset---2-hidden-layers}}

Figure \ref{fig:airfoil-median-fitness-30gens} shows both the mean and median of the best fitness for each generation on both the test and training set of the airfoil dataset for using 2 hidden layers inside the DAE-LSTM networks. The results are aggregated for \(10\) individuals runs per algorithm and also include regular GP as a benchmark. For all three algorithms an improvement in the best found fitness can be observed during the evolutionary search but the results show that DAE-GP based algorithms, on average, find solutions with much higher fitness as traditional GP.

The first interesting observation from this experiment is, that the pre-trained DAE-GP shows better mean and median performance than regular DAE-GP for both the training and the testing set in overall fitness. Regarding the presence of overfitting, both DAE-GP variants show only a very small gap between the testing and training fitness which indicates a good generalization behaviour.

\begin{figure}[c]

{\centering \includegraphics[width=0.9\linewidth]{./img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/mean_median_fitness_byGens} 

}

\caption{Best Fitness over 30 Generations - Airfoil (2HL)}\label{fig:airfoil-median-fitness-30gens}
\end{figure}

The distribution of final fitness scores for pre-trained and regular DAE-GP are visualized as boxplots in figure \ref{fig:airfoil-distribution-fitness-30gens}. The distribution for pre-trained DAE-GP shows to be preferable to that of regular DAE-GP on both sets since it shows a lower median (med.) as well as less dispersion measured by the standard deviation (std.).

\begin{figure}[c]

{\centering \includegraphics[width=0.8\linewidth]{./img/airfoil_2hl_maxIndSize_fullRun_30gens/final_fit_boxplot} 

}

\caption{Best Fitness after 30 Generations - Airfoil (2HL)}\label{fig:airfoil-distribution-fitness-30gens}
\end{figure}

Since previous work by {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]} demonstrated that DAE-GP was capable of producing solutions that are much smaller in tree size that regular GP, I also studied the influence of pre-training on the size of solutions found by DAE-GP.

Since {[}\protect\hyperlink{ref-dae-gp_2022_symreg}{17}{]} shows, that DAE-GP finds higher quality that are also smaller in size than those found by average GP for a given number of 10.000 fitness evaluations, I also studied the differences in average solution size between pre-trained and regular DAE-GP. Figure \ref{fig:airfoil-size-combined-30gens} shows the median of both the average size of individuals inside the population as well as the size of the currently best performing solution inside the population. It can be observed that both DAE-GP variants strongly reduce the average solutions size already in the first generations and that the average solution size inside the population constantly stays at this very low level for the rest of the search.

\begin{figure}[c]

{\centering \includegraphics[width=0.8\linewidth]{./img/airfoil_2hl_maxIndSize_fullRun_30gens/size_combined_median} 

}

\caption{Median Solution Size over 30 Generations - Airfoil (2HL)}\label{fig:airfoil-size-combined-30gens}
\end{figure}

The experiments was repeated using the same setup with the only difference being a reduction from two hidden layers to 1 hidden layer.

\begin{figure}
\centering
\includegraphics{./img/airfoil_1hl_maxIndSize_fullRun_30gens/mean_median_fitness_byGens.png}
\caption{Best Fitness over 30 Generations - 1 hidden Layer - Airfoil}
\end{figure}

\ldots{}

To gather more confidence in the previous results, the pre-training strategy for DAE-GP was tested for three additional real world symbolic regression problems taken from {[}\protect\hyperlink{ref-machine_learning_repo}{2}{]}.\footnote{see table \ref{tab:full-run-realWorldSymReg-problems}}

Table X summarizes the median best fitness achieved after running each algorithm for 10 runs using a population size of 500 individuals for 30 generations.\footnote{P-Values are marked with asteriks (1,2,3) for (\(\alpha\))-levels of \(0.1\), \(0.05\) and \(0.01\)}. The median best fitness over all 30 generations are visualized in figure X.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1932}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1705}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0795}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1477}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1477}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1023}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1591}}@{}}
\caption{\label{tab:full-run-realWorldSymReg-fitness}Median Best Fitness - Symbolic Regression}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hidden-Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Set
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hidden-Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Set
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Airfoil & 1 & Train & \textbf{33.3674} & 35.3454 & 0.02** & 0.64 \\
& 1 & Test & \textbf{33.7271} & 35.612 & 0.14 & 0.40 \\
Airfoil & 2 & Train & 17.4762 & \textbf{14.7233} & 0.31 & -0.28 \\
& 2 & Test & 17.5299 & \textbf{14.7603} & 0.57 & -0.16 \\
Boston\_Housing & 2 & Train & 8.1922 & \textbf{8.0763} & 0.29 & -0.29 \\
& 2 & Test & 8.0501 & \textbf{7.9692} & 0.71 & -0.11 \\
Energy(Cooling) & 2 & Train & \textbf{4.5271} & 4.7782 & 0.02** & 0.63 \\
& 2 & Test & \textbf{4.67} & 4.9196 & 0.29 & 0.29 \\
Concrete & 2 & Train & 17.0052 & \textbf{16.9085} & 0.97 & 0.02 \\
& 2 & Test & \textbf{16.5845} & 16.9298 & 0.52 & 0.18 \\
\end{longtable}

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight]{~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_fitness_byGens.png}
\caption{Median Best Fitness - Symbolic Regression}
\end{figure}

\hypertarget{solution-size-in-symbolic-regression}{%
\subsection{Solution Size in Symbolic Regression}\label{solution-size-in-symbolic-regression}}

Next I studied the size of solutions based on using a pre-training strategy. Figure X shows the median size of the best solutions aggregated of all benchmark problems for symbolic regression

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight]{~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_bestSol_size_byGens.png}
\caption{Median Solution Size - Symbolic Regression}
\end{figure}

Table X summarizes the results of the statistical tests.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1549}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1549}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1690}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1127}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1831}}@{}}
\caption{\label{tab:full-run-realWorldSymReg-best-solution-size}Median Solution Size - Symbolic Regression}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Airfoil & 1 & 12 & \textbf{10.6} & 0.46 & -0.20 \\
Airfoil & 2 & 17.4762 & \textbf{14.7233} & 0.31 & -0.28 \\
Boston\_Housing & 2 & 8.1922 & \textbf{8.0763} & 0.29 & -0.29 \\
Energy(Cooling) & 2 & \textbf{4.5271} & 4.7782 & 0.02** & 0.63 \\
Concrete & 2 & 17.0052 & \textbf{16.9085} & 0.97 & 0.02 \\
\end{longtable}

\hypertarget{population-diversity}{%
\subsection{Population Diversity}\label{population-diversity}}

Another interesting metric to examine in EC based metaheuristics is the diversity of the current generations population. Since the individual solutions in DAE-GP are computer programs in the form ob parse trees, I selected the normalized Levenshtein edit distance as my primary metric for tracking population diversity. figure X shows how the median population diversity develops over the full number of generations for all benchmark problems.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight]{~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_levDistance_byGen.png}
\caption{Median Population Diversity over Generations}
\end{figure}

Testing for statistical differences in the underlying distributions yields the P-Values listed in table X.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2267}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1600}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1333}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1733}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1200}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1867}}@{}}
\caption{\label{tab:full-run-realWorldSymReg-popDiversity}Median Population Diversity over Generations - Symbolic Regression}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Airfoil & 1 & \textbf{0.34} & 0.49 & 0.02** & 0.36 \\
Airfoil & 2 & \textbf{0.91} & 0.93 & 0.00*** & 0.88 \\
Boston\_Housing & 2 & \textbf{0.94} & 0.95 & 0.00*** & 0.82 \\
Energy(Cooling) & 2 & \textbf{0.93} & 0.94 & 0.00*** & 0.80 \\
Concrete & 2 & \textbf{0.93} & 0.94 & 0.00*** & 0.88 \\
\end{longtable}

\hypertarget{training-epochs-per-generation}{%
\subsection{Training epochs per Generation}\label{training-epochs-per-generation}}

One important argument for using pre-training is the reduction of runtime and computational ressources for DAE-GP. An important factor in this, is the number of epochs that each DAE-LSTM has to be trained per generation. I studied the median number of training epochs that each generations DAE-LSTM \(M_g\) (excluding the pre-training DAE-LSTM model \(\hat{M}\)) was trained until the training was stopped. The results are visualized in figure X and summarized in table X

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight]{~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_epochsTrained.png}
\caption{Median Number of Training Epochs per Generation - Symbolic Regression}
\end{figure}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2222}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1111}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1944}}@{}}
\caption{\label{tab:full-run-realWorldSymReg-epochsPerGen}Median Number of Training Epochs per Generation - Symbolic Regression}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hid.Layers
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DAE-GP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pre-Trained
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
P-Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cliffs-Delta
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Airfoil & 1 & 190.93 & \textbf{60} & 0.00*** & -0.94 \\
Airfoil & 2 & 151.87 & \textbf{64.95} & 0.00*** & -0.88 \\
Boston\_Housing & 2 & 182.84 & \textbf{67.27} & 0.00*** & -0.88 \\
Energy\_Cooling & 2 & 169.39 & \textbf{69.87} & 0.00*** & -0.90 \\
concrete & 2 & 171.69 & \textbf{64.9} & 0.00*** & -0.92 \\
\end{longtable}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\ldots{}

\hypertarget{limitations-and-open-questions}{%
\section{Limitations and open Questions}\label{limitations-and-open-questions}}

\hypertarget{further-questions}{%
\subsection{Further Questions}\label{further-questions}}

{[}\protect\hyperlink{ref-pmlr-v5-erhan09a}{3}{]}: Pre-Training effect especially usefull for lower-level layers -\textgreater{} Only adapt weights from low layers or embedd the pre-trained model?

Different pre-training strategies?

\newpage

\hypertarget{I}{%
\section*{References}\label{I}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-chollet2015keras}{}}%
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{Francois Chollet and others. 2015. Keras. Retrieved from \url{https://github.com/fchollet/keras}}

\leavevmode\vadjust pre{\hypertarget{ref-machine_learning_repo}{}}%
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{Dheeru Dua and Casey Graff. 2017. {UCI} machine learning repository. Retrieved from \url{http://archive.ics.uci.edu/ml}}

\leavevmode\vadjust pre{\hypertarget{ref-pmlr-v5-erhan09a}{}}%
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal Vincent. 2009. The difficulty of training deep architectures and the effect of unsupervised pre-training. In \emph{Proceedings of the twelth international conference on artificial intelligence and statistics} (Proceedings of machine learning research), PMLR, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 153--160. Retrieved from \url{https://proceedings.mlr.press/v5/erhan09a.html}}

\leavevmode\vadjust pre{\hypertarget{ref-DEAP_JMLR2012}{}}%
\CSLLeftMargin{{[}4{]} }%
\CSLRightInline{Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, and Christian Gagné. 2012. {DEAP}: Evolutionary algorithms made easy. \emph{Journal of Machine Learning Research} 13, (July 2012), 2171--2175.}

\leavevmode\vadjust pre{\hypertarget{ref-HAN2021225}{}}%
\CSLLeftMargin{{[}5{]} }%
\CSLRightInline{Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. 2021. Pre-trained models: Past, present and future. \emph{AI Open} 2, (2021), 225--250. DOI:https://doi.org/\url{https://doi.org/10.1016/j.aiopen.2021.08.002}}

\leavevmode\vadjust pre{\hypertarget{ref-ae_orig}{}}%
\CSLLeftMargin{{[}6{]} }%
\CSLRightInline{G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. \emph{Science} 313, 5786 (2006), 504--507. DOI:https://doi.org/\href{https://doi.org/10.1126/science.1127647}{10.1126/science.1127647}}

\leavevmode\vadjust pre{\hypertarget{ref-lstm_orig}{}}%
\CSLLeftMargin{{[}7{]} }%
\CSLRightInline{Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. \emph{Neural computation} 9, (December 1997), 1735--80. DOI:https://doi.org/\href{https://doi.org/10.1162/neco.1997.9.8.1735}{10.1162/neco.1997.9.8.1735}}

\leavevmode\vadjust pre{\hypertarget{ref-Koza1993GeneticP}{}}%
\CSLLeftMargin{{[}8{]} }%
\CSLRightInline{John R. Koza. 1993. Genetic programming - on the programming of computers by means of natural selection. In \emph{Complex adaptive systems}.}

\leavevmode\vadjust pre{\hypertarget{ref-survey_transfer_learning}{}}%
\CSLLeftMargin{{[}9{]} }%
\CSLRightInline{Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. \emph{IEEE Transactions on Knowledge and Data Engineering} 22, 10 (2010), 1345--1359. DOI:https://doi.org/\href{https://doi.org/10.1109/TKDE.2009.191}{10.1109/TKDE.2009.191}}

\leavevmode\vadjust pre{\hypertarget{ref-10.1007ux2f978-3-540-24621-3_22}{}}%
\CSLLeftMargin{{[}10{]} }%
\CSLRightInline{Grégory Paris, Denis Robilliard, and Cyril Fonlupt. 2004. Exploring overfitting in genetic programming. In \emph{Artificial evolution}, Springer Berlin Heidelberg, Berlin, Heidelberg, 267--277.}

\leavevmode\vadjust pre{\hypertarget{ref-harmless_overfitting_eda}{}}%
\CSLLeftMargin{{[}11{]} }%
\CSLRightInline{Malte Probst and Franz Rothlauf. 2020. Harmless overfitting: Using denoising autoencoders in estimation of distribution algorithms. \emph{Journal of Machine Learning Research} 21, 78 (2020), 1--31. Retrieved from \url{http://jmlr.org/papers/v21/16-543.html}}

\leavevmode\vadjust pre{\hypertarget{ref-design_of_modern_heuristics}{}}%
\CSLLeftMargin{{[}12{]} }%
\CSLRightInline{Franz Rothlauf. 2011. \emph{Design of modern heuristics: Principles and application}. DOI:https://doi.org/\href{https://doi.org/10.1007/978-3-540-72962-4}{10.1007/978-3-540-72962-4}}

\leavevmode\vadjust pre{\hypertarget{ref-sampling_err_gp}{}}%
\CSLLeftMargin{{[}13{]} }%
\CSLRightInline{Dirk Schweim, David Wittenberg, and Franz Rothlauf. 2021. On sampling error in genetic programming. \emph{Natural computing} 2021, (2021). DOI:https://doi.org/\url{http://doi.org/10.25358/openscience-5820}}

\leavevmode\vadjust pre{\hypertarget{ref-dae_orig2008}{}}%
\CSLLeftMargin{{[}14{]} }%
\CSLRightInline{Pascal Vincent, Hugo Larochelle, Y. Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In \emph{Proceedings of the 25th international conference on machine learning} (ICML '08), Association for Computing Machinery, New York, NY, USA, 1096--1103. DOI:https://doi.org/\href{https://doi.org/10.1145/1390156.1390294}{10.1145/1390156.1390294}}

\leavevmode\vadjust pre{\hypertarget{ref-weigend1994overfitting}{}}%
\CSLLeftMargin{{[}15{]} }%
\CSLRightInline{Andreas Weigend. 1994. On overfitting and the effective number of hidden units. In \emph{Proceedings of the 1993 connectionist models summer school}, 335--342.}

\leavevmode\vadjust pre{\hypertarget{ref-daegp_explore_exploit}{}}%
\CSLLeftMargin{{[}16{]} }%
\CSLRightInline{David Wittenberg. 2022. Using denoising autoencoder genetic programming to control exploration and exploitation in search. In \emph{Genetic programming}, Springer International Publishing, Cham, 102--117.}

\leavevmode\vadjust pre{\hypertarget{ref-dae-gp_2022_symreg}{}}%
\CSLLeftMargin{{[}17{]} }%
\CSLRightInline{David Wittenberg and Franz Rothlauf. 2022. Denoising autoencoder genetic programming for real-world symbolic regression. In \emph{Proceedings of the genetic and evolutionary computation conference companion} (GECCO '22), Association for Computing Machinery, New York, NY, USA, 612--614. DOI:https://doi.org/\href{https://doi.org/10.1145/3520304.3528921}{10.1145/3520304.3528921}}

\leavevmode\vadjust pre{\hypertarget{ref-dae-gp_2020_rtree}{}}%
\CSLLeftMargin{{[}18{]} }%
\CSLRightInline{David Wittenberg, Franz Rothlauf, and Dirk Schweim. 2020. DAE-GP: Denoising autoencoder LSTM networks as probabilistic models in estimation of distribution genetic programming. In \emph{Proceedings of the 2020 genetic and evolutionary computation conference} (GECCO '20), Association for Computing Machinery, New York, NY, USA, 1037--1045. DOI:https://doi.org/\href{https://doi.org/10.1145/3377930.3390180}{10.1145/3377930.3390180}}

\end{CSLReferences}

\end{document}
