---
title: "Pre-Trained Denoising Autoencoders Long Short-Term Memory Networks as probabilistic
  Models for Estimation of Distribution Genetic Programming"
author: "Roman Hoehn"
date: 'Datum: 25.01.2023'
output:
  beamer_presentation:
    theme: "Madrid"
    # colortheme: "beaver"
    fonttheme: structurebold
    fig_caption: no
    number_sections: yes
    slide_level: 2
subtitle: 'Kolloquium zur Masterarbeit im M.Sc. Wirtschaftspädagogik '
link-citations: yes
institute: JGU Mainz
toc: yes
bibliography: ref/ref.bib
csl: "csl/harvard-cite-them-right.csl"
lang: de
# includes:
#   in_header: header_files/slides.tex
---


```{r setup, include=FALSE}
library(kableExtra)
library(magrittr)
library(reticulate)


knitr::opts_chunk$set(
  echo = TRUE,
  fig.pos = "c",
  fig.align="center"
  )



reticulate::use_python("/Users/rmn/miniconda3/envs/dataScience/bin/python")

```


 ```{bash recompile_results, echo=TRUE, eval=TRUE, include=FALSE}

# set eval to TRUE to recompile all results, plots etc.
# Warning: This will take some time...


# compile mermaid diagrams
# ---
WIDTH=3200
HEIGHT=2400
THEME="neutral"
BACKGROUND="white"

if [ ! -d "./img/flowcharts" ]; then
  mkdir -p "./img/flowcharts"
fi


for file in ./mermaid/*.mmd
do
  mmdc -i "$file" -o ./img/flowcharts/$(basename "$file" .mmd).png -t $THEME -w $WIDTH -H $HEIGHT -b $BACKGROUND 
done

# Airfoil First Gen
# ---
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/airfoil_firstGen/re_evaluate_airfoilFirstGen.sh


# Real World SymReg Full Runs
# ---

# airfoil
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/airfoil_1hl_maxIndSize_fullRun_30gens/re_evaluate_airfoil_1hl_maxIndSize_30gen.sh

bash /Users/rmn/masterThesis/eda-gp-2020/experiments/airfoil_2hl_maxIndSize_fullRun_30gens/re_evaluate_airfoil_2hl_maxIndSize_30gen.sh

# energy (cooling)
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/energyCooling_2hl_maxIndSize_fullRun_30gens/re_evaluate_energyCooling_2hl_maxIndSize_fullRun_30gens.sh

# boston housing
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/bostonHousing_2hl_maxIndSize_fullRun_30gens/re_evaluate_bostonHousing_2hl_maxIndSize_fullRun_30gens.sh

# concrete
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/concrete_2hl_maxIndSize_fullRun_30gens/re_evaluate_concrete_2hl_maxIndSize_fullRun_30gens.sh

# summarize csv files and plots for full run experiments
/Users/rmn/miniconda3/envs/dataScience/bin/python /Users/rmn/github/master_thesis/data/summarize_data.py

# 2nd generation Pre-Training
bash /Users/rmn/masterThesis/eda-gp-2020/experiments/2ndGenPT_airfoil_2hl_150hn_fullRun/re_evaluate_2ndGenPT_airfoil_2hl_150hn_fullRun.sh

# create Plots from pstats (cProfile)
JUPYTER="/Users/rmn/miniconda3/envs/dataScience/bin/jupyter"
$JUPYTER nbconvert --to html --execute /Users/rmn/masterThesis/eda-gp-2020/cProfiler/eval.ipynb

echo Done recompiling Results, Plots, Diagrams, etc!

```


# Einleitung

## Forschungsfrage

*Kann das Suchverhalten der Denoising Autoencoder Genetic Programming (DAE-GP) Metaheuristik durch den Einsatz einer Pre-Training Strategie optimiert werden?*


Welchen Effekt hat Pre-Training auf:

  1. die Qualität der gefundenen Programme (Fitness/Programmlänge)?
  2. die Populationsdiversität?
  3. das Laufzeitverhalten?
  
Anwendungsgebiet: Symbolische Regression, Fokus auf Airfoil Datensatz^[@machine_learning_repo]
  


# Denoising Autoencoder Genetic Programming 

## Übersicht

* Metaheuristik basierend auf genetischer Programmierung (GP)
* Ersetzung der Variationsoperatoren von GP durch künstliche, neuronale Netze zur Optimierung des Suchverhaltens^[@dae-gp_2020_rtree]
* Variante des Estimation of Distribution-GP (EDA-GP)
* Einsatz von Pre-Training in mehreren Publikationen als möglicher Weg für eine weitere Optimierung vorgeschlagen^[@dae-gp_2022_symreg] ^[@daegp_explore_exploit]


## Estimation of Distribution Algorithmen^[@edaOrig1996] ^[@design_of_modern_heuristics] (EDA)

* Entwicklung neuer Rekombinationsoperatoren für evolutionäre Algorithmen basierend auf dem Einsatz von probabilistischen Modellen
* Hypothese: Problemspezifische Abhängigkeiten zwischen Entscheidungsvariablen können bei der Erzeugung neuer Individuen besser berücksichtigt werden als bei traditionellen Rekombinationsoperatoren


## Denoising Autoencoder Estimation of Distribution Algorithmen (DAE-EDA)

Idee: Einsatz von Denoising Autoencoders^[@dae_orig2008] (DAE) als probabilistisches Modell für genetische Algorithmen ^[@harmless_overfitting_eda]

2 Phasen Ansatz:

  1. Model Building: Modell "lernt" die Eigenschaften von ausgewählten Lösungen hoher Güte durch das Trainieren eines DAE
  2. Model Sampling: Neue Lösungen werden erzeugt durch das propagieren von bestehenden, mutierten Lösungen durch das erlernte Modell 
  

## Denoising Autoencoder Genetic Programming (DAE-GP)

* Adaptierung des DAE-EDA Algorithmus auf GP
* Schwierigkeit: Variation von GP Lösungen in Länge und Baumstruktur
* seq2seq learning Problem: Einsatz von DAE - Long Short Term Memory Netzwerken (DAE-LSTM)

## DAE-GP Ablauf

```{r echo=FALSE, out.height="90%", out.width="50%"}
knitr::include_graphics("./img/flowcharts/dae-gp.png")
```


## Pre-Training

Idee: Modelle werden vor ihrem eigentlichen Einsatz zum Lösen eines Problems auf möglichst großen Datensätzen vortrainiert

Mögliche Vorteile durch Pre-Training^[@pmlr-v5-erhan09a]: 

1. Geringere Bedarf an Trainings Daten für vortrainierte Modelle
2. Reduktion der Trainingszeiten/Laufzeiten
3. Verbesserung der Güte des Modells
4. Verbessertes Generalisierungsverhalten des Modells


# Aktueller Forschungsstand

## Generalisiertes Royal Tree Problem^[@dae-gp_2020_rtree] (GRT):

  * Einfaches Suchproblem mit hoher Lokalität
  * DAE-GP erzeugt durch Model Sampling Lösungskandidaten mit höherer Fitness als GP 
  * Hohe Güte der erzeugten Lösungskandidaten resultiert in besserer Performance
  * Perfomance Vorteil steigt mit zunehmender Komplexität des GRT Problems
  
  
## Symbolische Regression^[@dae-gp_2022_symreg]:

  * Airfoil Datensatz für Real-World symbolische Regression
  * DAE-GP erzeugt für eine vorgegebene Anzahl von Fitness Evaluationen im Vergleich zu GP:
  
    1. Lösungen mit höherer Fitness
    2. Lösungen  mit geringerer Größe


## Pre-Training für Denoising Autoencoders^[@pmlr-v5-erhan09a]

### Positive Wirkung von Pre-Training auf DAE

1. Gesteigerte Modell Performance (sinkender Testfehler)
2. Besserer Generalisierungsfähigkeit
3. Erhöhter Robustness des Algorithmus (sinkende Varianz des Testfehlers)


### Einfluss der Modell Architektur

* Positiver Effekt steigt mit zunehmender Komplexitität des DAE
* Je mehr versteckte Layer oder Neuronen pro verstecktem Layer vorhanden sind, desto besserer Effekt des Pre-Trainings
* Für sehr kleine DAE, zeigt Pre-Training jedoch inverse, negative Auswirkung auf die Modell Performance



# Implementation

## Überblick

Gewählte Pre-Training Strategie: 

* (Klassisches) Pre-Training: Einmaliges Pre-Training eines DAE-LSTM mit einer großen Population aus Lösungskandidaten $\hat{P}_{train}$ (ramped Half and Half Initialisierung)

* Trainingsmethode Early Stopping: Abbruch des Trainings sobald der Testfehler für eine seperate Population $\hat{P}_{test}$ konvergiert

*Ausschluss weiterer Pre-Training Strategien wie Re-Use Learning, Few-Shot Learning*

## Pre-Trained DAE-GP Ablauf

```{r, echo=FALSE, out.height="90%", out.width="50%"}
knitr::include_graphics("./img/flowcharts/pt-dae-gp.png")
```


## Herausforderung - DAE-LSTM Dimension

* Im Verlaufe des DAE-GP Algorithmus passt sich die Dimension der DAE-LSTM Netzwerke $M_{g}$ dynamisch an die Größe der Individuen der aktuellen Population an (Dimension nimmt i.d.R. stark ab innerhalb der ersten Generationen)

Der Einsatz der Pre-Training Strategie ist nur möglich bei einer konstanten Anzahl an Neuronen. Zwei Ansätze verfolgt:

1. DAE-GP mit dynamischer Anpassung, Pre-Trained DAE-GP mit konstanter Anzahl (maximale Länge innerhalb von $\hat{P}$)
2. Beide Algorithmen mit einer fixen Anzahl von versteckten Neuronen


<!--
## DAE-GP - Entwicklung DAE-LSTM Dimension

![Airfoil - Entwicklung DAE-LSTM Dimension](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens/median_hiddenNeurons_perGen.png){height=90%}

--->



# Untersuchung des Suchverhaltens bei symbolischer Regression

## Fragestellung

Welchen Einfluss hat die Verwendung einer Pre-Training Strategie auf das Suchverhalten von DAE-GP bei der Anwendung auf symbolische Regressionsprobleme?

Aufbau: Betrachtung des Suchverhalten über je $10$ Gesamtduchläufe am Airfoil Datensatz für DAE-GP und pre-trained DAE-GP

Fokus insbesondere auf:

* Lösungsqualität (Fitness)
* Größe der gefundenen Lösungen (Anzahl an Knoten)
* Populationsdiversität
* Laufzeit

## Hyperparameter

```{r airfoil_fullRun_2hl_maxIndSize_params, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"
knitr::kable(
  read.csv("./tables/airfoil_fullRun_2hl_maxIndSize_params"),
)%>% row_spec(0,bold=TRUE )%>% kable_styling(latex_options = "scale_down", font_size = 6)
```


## Airfoil - Entwicklung der Fitness über Generationen

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_fitness_byGens.png")
```

<!-- ![Airfoil Datensatz - Fitness über Generationen](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_fitness_byGens.png){height=90%} -->


## Airfoil - Verteilung der finalen Fitness

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png")
```

<!-- ![Airfoil Datensatz - Verteilung der finale Fitness](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png){height=90%} -->

## Airfoil - Größe der besten Lösungskandidaten

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_best_Size_byGens.png")
```

<!-- ![Airfoil Datensatz - Größe der besten Lösung über Generationen](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_best_Size_byGens.png){height=90%} -->

## Airfoil - Populationsdiversität

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_popDiversity_byGens_levOnly_withGP.png")
```

<!-- ![Airfoil - Populationsdiversität](/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/median_popDiversity_byGens_levOnly_withGP.png){height=90%} -->


## Airfoil - Laufzeit (Gesamt)

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_runtime.png")
```

<!-- ![Airfoil - Laufzeit](/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_runtime.png){height=90%} -->


## Airfoil - Laufzeit (Trainingsepochen)

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_training_epochs.png")
```

<!-- ![Airfoil - Trainingsepochen](/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_training_epochs.png){height=90%} -->

## Airfoil - Laufzeit (Sampling Time)

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_sampleTime.png")
```

<!-- ![Airfoil - Sampling-Dauer](/Users/rmn/github/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_TimeBenchmark/median_sampleTime.png){height=90%} -->

<!-- ## Airfoil - Laufzeit (Profiling) -->

<!-- ```{r, echo=FALSE, out.height="85%", out.width="80%"} -->
<!-- knitr::include_graphics("/Users/rmn/github/master_thesis/img/Airfoil_RuntimeProfiling/Runtime_Bar.png") -->
<!-- ``` -->

<!-- ![Airfoil - Profiling](/Users/rmn/github/master_thesis/img/Airfoil_RuntimeProfiling/Runtime_Bar.png){height=90%} -->



## Kontrollexperiment: Reduzierung der DAE-LSTM Dimension für Airfoil Datensatz

Frage: Welchen Einfluss hat die Reduktion der DAE-LSTM auf 1 hidden Layer (HL)?

Erwartung: Negativer Einfluss von Pre-Training auf die Güte der gefundenen Lösungen (Fitness/Größe) durch sinkenden Modell Performance der DAE-LSTMs^[@pmlr-v5-erhan09a]


## Airfoil - Entwicklung der Fitness (1 HL)


```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/median_fitness_byGens.png")
```

<!-- ![Airfoil - Entwicklung der Fitness über Generationen (1 HL)](~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/median_fitness_byGens.png){height=85%} -->

## Airfoil - Verteilung der finalen Fitness (1HL)

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png")
```

<!-- ![Airfoil - Verteilung der finalen Fitness (1HL)](~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png){height=75%} -->

## Airfoil - Größe der besten Lösungskandidaten(1HL)

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/median_Size_byGens.png")
```

<!-- ![Airfoil - Größe der besten Lösung über Generationen (1HL)](~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/median_Size_byGens.png){height=90%} -->


## Anwendung auf weitere Datensätzen

Airfoil Datensatz: Erste Ergebnis deuten bei einer ausreichenden Anzahl von Hidden Layern auf einen postitiven Effekt der Pre-Training Strategie hin:

* Höhere Fitness der gefundenen Lösungen
* Kleinere Größe der gefundenen Lösungen

Daher: Ausweitung des Experiments auf weitere Datensätze

## Übersicht Datensätze

```{r full_run_realWorldSymReg_problems, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"
knitr::kable(
  read.csv("./tables/full_run_realWorldSymReg_problems.csv"),
  #caption="Overview - Real World Symbolic Regression Benchmark Problems"
)%>% row_spec(0,bold=TRUE ) %>% kableExtra::kable_styling(latex_options = "hold_position")
```


## Median Fitness - Symbolische Regression

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_fitness_byGens.png")
```
<!-- ![Übersicht Median Fitness - Symbolische Regression](~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_fitness_byGens.png){height=80%} -->


## Fitness (Median) - Symbolische Regression


```{r full_run_realWorldSymReg_fitness, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_final_fit_median.csv",
  sep=",",
  colClasses = c("character", "numeric", "character", "numeric", "numeric", "character", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],2)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],2))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],2)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],2))
  }
}
# shorten column names
col_names <- c("Problem", "Hidden-Layers", "Set", "DAE-GP", "Pre-Trained", "P-Value", "Cliffs-Delta")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% 
  kable_styling(latex_options = "scale_down", font_size = 7)

```



## Lösungsgröße (Median) - Symbolische Regression

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_bestSol_size_byGens.png")
```

<!-- ![Übersicht Lösungsgröße (Median) - Symbolische Regression](~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_bestSol_size_byGens.png){height=80%} -->


## Auswertung Lösungsgröße (Median) - Symbolische Regression

```{r full_run_realWorldSymReg_best_solution_size, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_best_size_median.csv",
  sep=",",
  colClasses = c("character", "numeric", "numeric", "numeric", "character", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],2)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],2))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],2)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],2))
  }
}
# shorten column names
col_names <- c("Problem", "Hid.Layers", "DAE-GP", "Pre-Trained", "P-Value", "Cliffs-Delta")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% 
  kable_styling(latex_options = "scale_down", font_size = 7)

```


## Populationsdiversität (Median) - Symbolische Regression

```{r, echo=FALSE, out.height="80%", out.width="75%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_levDistance_byGen.png")
```

<!-- ![Übersicht Populationsdiversität (Median) - Symbolische Regression](~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_levDistance_byGen.png){height=80%} -->

## Auswertung Populationsdiversität (Median) - Symbolische Regression


```{r full_run_realWorldSymReg_popDiversity, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_median_LevDistance.csv",
  sep=",",
  colClasses = c("character", "numeric", "numeric", "numeric", "character", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],2)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],2))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],2)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],2))
  }
}
# shorten column names
col_names <- c("Problem", "Hid.Layers", "DAE-GP", "Pre-Trained", "P-Value", "Cliffs-Delta")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% 
  kable_styling(latex_options = "scale_down", font_size = 7)

```



## Trainingsepochen pro Generation (Median) - Symbolische Regression

```{r, echo=FALSE, out.height="80%", out.width="75%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_epochsTrained.png")
```

<!-- ![Übersicht Trainingsepochen pro Generation (Median) - Symbolische Regression](~/masterThesis/master_thesis/img/symbolicRegressionSummarized/median_epochsTrained.png){height=80%} -->

## Auswertung Trainingsepochen pro Generation (Median) - Symbolische Regression


```{r full_run_realWorldSymReg_epochsPerGen, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_mean_epochsTrained.csv",
  sep=",",
  colClasses = c("character", "numeric", "numeric", "numeric", "character", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],2)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],2))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],2)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],2))
  }
}
# shorten column names
col_names <- c("Problem", "Hid.Layers", "DAE-GP", "Pre-Trained", "P-Value", "Cliffs-Delta")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% 
  kable_styling(latex_options = "scale_down", font_size = 7)

```


## Zusammenfassung- Symbolische Regression

Keine Evidenz für einen statistisch signifikanten Einfluss von Pre-Training auf die erzielte Lösungsqualität oder die erzielte Lösungsgröße in den durchgeführten Experimenten!

Vorteile durch Pre-Training:

1. Signifikante Erhöhung der Populationsdiversität in allen Experimenten
2. Signifikante Reduktion der Anzahl an Trainingsepochen pro Generationen in allen Experimenten


# Alternative Pre-Training Strategien

## Eingeschobenes Pre-Training in der 2. Generation

Ansatz: Deutliche Fitnessverbesserung finden oft bereits in der ersten Generationen der evolutionären Suche statt

* Sampling des DAE-LSTMs der ersten Generation $M_1$ zur Initialisierung der Pre-Training Population $\hat{P}$
* Training des pre-training modells $\hat{M}$ mit $\hat{P}$ nach Abschluss der ersten Generation
* DAE-LSTM Modelle werden ab Generation 2 mit den Parametern von $\hat{M}$ initialisiert


## Grow Initialisierung der Pre-Training Population 

Ansatz: Initialisierung von $\hat{P}$ ausschließlich mit der Grow Methode^[@Koza1993GeneticP] anstelle von ramped Half and Half 

Hintergrund: Vermutlich geringer Informationsgewinn aus dem Lernen von vollen Lösungsbäume aus $\hat{P}$ mit den gesampleten Populationen der späteren Generationen


## Fitness (Median) - Alternative Ansätze

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/median_fitness_byGens.png")
```

<!-- ![Übersicht Fitness pro Generation (Median) - Weitere Ansaetze](~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/median_fitness_byGens.png){height=80%} -->


<!-- ## Auswertung Fitness (Median) - Alternative Ansätze -->

<!-- ```{r full_run_PT_Strat_Fitness, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(dplyr) -->

<!-- # Create the data frame from csv -->
<!-- df <- read.csv( -->
<!--   "/Users/rmn/github/master_thesis/data/summary_median_Fitness_lastGen_PT_Strategies_Airfoil_2hl_150hn.csv", -->
<!--   sep="," -->
<!-- ) -->

<!-- col_names <- c("Set", "Standard-PT", "2ndGen", "P-Val(1)" , "GrowInit", "P-Val(2)") -->

<!-- knitr::kable( -->
<!--  df, -->
<!--  format = "markdown", -->
<!--  align = "l", -->
<!--  digits=2, -->
<!--  col.names = col_names, -->
<!--  #caption = "Symbolic Regression Overview - Mean final Fitness" -->
<!-- ) %>%  -->
<!--   kable_styling(latex_options = "scale_down", font_size = 7) -->

<!-- ``` -->

## Lösungsgröße (Median) - Alternative Ansätze

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/median_Size_best_solution.png")
```

<!-- ![Übersicht Lösungsgröße der besten Individuen (Median) - Weitere Ansaetze](~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/median_Size_best_solution.png){height=80%} -->


<!-- ## Auswertung Lösungsgröße (Median) - Alternative Ansätze -->

<!-- ```{r full_run_PT_Strat_BestSize, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(dplyr) -->

<!-- # Create the data frame from csv -->
<!-- df <- read.csv( -->
<!--   "/Users/rmn/github/master_thesis/data/summary_median_Size_best_Solution_PT_Strategies_Airfoil_2hl_150hn.csv", -->
<!--   sep="," -->
<!-- ) -->

<!-- col_names <- c("Standard-PT", "2ndGen", "P-Val(1)" , "GrowInit", "P-Val(2)") -->

<!-- knitr::kable( -->
<!--  df, -->
<!--  format = "markdown", -->
<!--  align = "l", -->
<!--  digits=2, -->
<!--  col.names = col_names, -->
<!--  #caption = "Symbolic Regression Overview - Mean final Fitness" -->
<!-- ) %>%  -->
<!--   kable_styling(latex_options = "scale_down", font_size = 7) -->

<!-- ``` -->

## Populationsdiversität (Median) - Alternative Ansätze

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/median_lev_div_overGens.png")
```

<!-- ![Übersicht Populationsdiversität (Median) - Weitere Ansaetze](~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/mean_popDiversity_byGens_levOnly.png){height=80%} -->


<!-- ## Auswertung Populationsdiversität (Median) - Alternative Ansätze -->

<!-- ```{r full_run_PT_Strat_LevDiv, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(dplyr) -->

<!-- # Create the data frame from csv -->
<!-- df <- read.csv( -->
<!--   "/Users/rmn/github/master_thesis/data/summary_median_Norm._Levenshtein_Distance_PT_Strategies_Airfoil_2hl_150hn.csv", -->
<!--   sep="," -->
<!-- ) -->

<!-- col_names <- c("Standard-PT", "2ndGen", "P-Val(1)" , "GrowInit", "P-Val(2)") -->

<!-- knitr::kable( -->
<!--  df, -->
<!--  format = "markdown", -->
<!--  align = "l", -->
<!--  digits=2, -->
<!--  col.names = col_names, -->
<!--  #caption = "Symbolic Regression Overview - Mean final Fitness" -->
<!-- ) %>%  -->
<!--   kable_styling(latex_options = "scale_down", font_size = 7) -->

<!-- ``` -->
## Trainingsepochen (Median) - Alternative Ansätze

```{r, echo=FALSE, out.height="85%", out.width="80%"}
knitr::include_graphics("~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/mean_epochs_trained_perGen.png")
```

<!-- ![Übersicht Trainingsepochen (Median) - Weitere Ansaetze](~/masterThesis/master_thesis/img/PT_Strategies_airfoil_2hl_150hn_fullRun/mean_epochs_trained_perGen.png){height=80%} -->


<!-- ## Auswertung Trainingsepochen (Median) - Alternative Ansätze -->

<!-- ```{r full_run_PT_Strat_EpochsTrained, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(dplyr) -->

<!-- # Create the data frame from csv -->
<!-- df <- read.csv( -->
<!--   "/Users/rmn/github/master_thesis/data/summary_median_Training_Epochs_PT_Strategies_Airfoil_2hl_150hn.csv", -->
<!--   sep="," -->
<!-- ) -->

<!-- col_names <- c("Standard-PT", "2ndGen", "P-Val(1)" , "GrowInit", "P-Val(2)") -->

<!-- knitr::kable( -->
<!--  df, -->
<!--  format = "markdown", -->
<!--  align = "l", -->
<!--  digits=2, -->
<!--  col.names = col_names, -->
<!--  #caption = "Symbolic Regression Overview - Mean final Fitness" -->
<!-- ) %>%  -->
<!--   kable_styling(latex_options = "scale_down", font_size = 7) -->

<!-- ``` -->

# Fazit

## Vorteile

Der Einsatz von Pre-Training dient:

* der Erhöhung der Populationsdiversität (mögliche Strategie zur Kontrolle des Explore/Exploit Verhaltens von DAE-GP?)

* der Reduktion der Trainingsepochen pro Generation (Reduktion der Rechenleistung, insbesondere bei komplexen DAE-LSTM nützlich?)

## Nachteile

* DAE-GP verliert durch Einsatz von Pre-Training die Möglichkeit die Anzahl künstlicher Neuronen pro verstecktem Layer anzupassen

* Erhöhung der Rechenzeit/Ressourcen: DAE-LSTMs müssen ausreichend dimensioniert sein (2+ Hidden Layer), aktuelle Publikationen nutzen nur einen einzelnen Hidden Layer^[@dae-gp_2022_symreg] ^[@daegp_explore_exploit].

* Keine signifikanten Vorteile für die Qualität der gefundenen Lösungen

<!--

# Untersuchung des Generalisierungsverhalten 

## Fragestellung

Welchen Einfluss hat die Dimension der eingesetzten DAE-LSTMs auf das Generalisierungsverhalten von DAE-GP in Kombination mit Pre-Training?

Experimenteller Aufbau: 

* Ausschließliche Betrachtung des Rekonstruktionsfehlers der ersten Generation (Nutzung von seperaten Test- und Trainingspopulationen)
* DAE-LSTM Training erfolgt über eine fixe Anzahl von $1000$ Epochen (kein frühzeitiger Abbruch bei Konvergenz des Testfehlers)

Insgesamt 12 Subexperimente mit jeweils 10 Durchläufen ($120$ Gesamtdurchläufe):

* Variable Anzahl von Hidden Neurons (50, 100, 200) mit einem Hidden Layer
* Variable Anzahl von Hidden Layers (1, 2, 3) mit 100 Hidden Neurons
* (normales) DAE-GP und pre-trained DAE-GP


## Einfluss der Anzahl von Hidden Layers

![Airfoil - Erste Generation Median Trainingsfehler - Variable Anzahl Hidden Layer (2/2)](./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_layers_3plots.png){height=85%}


## Einfluss der Anzahl von Hidden Neurons

![Airfoil - Erste Generation Median Trainingsfehler - Variable Anzahl Hidden Neurons (2/2)](./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_neurons_3plots.png){height=85%}

-->


# References {.allowframebreaks} 







