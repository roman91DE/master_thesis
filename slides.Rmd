---
title: "Pre-Trained Denoising Autoencoders Long Short-Term Memory Networks as probabilistic Models for Estimation of Distribution Genetic Programming"
subtitle: "Kolloquium zur Masterarbeit im M.Sc. Wirtschaftspädagogik "
author: "Roman Hoehn"
institute: "Johannes Gutenberg-Universität Mainz"
date: "Datum: 25.01.2023"
fontsize: 11pt
link-citations: true
output:
  beamer_presentation:
    #theme: "Madrid"
    fonttheme: "structurebold"
    fig_width: 12
    fig_height: 10
    # keep_tex: true
    fig_caption: no
    number_sections: true
    slide_level: 2
toc: yes
bibliography: ref/ref.bib
csl: csl/harvard-cite-them-right.csl
includes:
      in_header: [header_files/slides.tex]

---


```{r setup, include=FALSE}
library(kableExtra)
library(magrittr)
library(reticulate)


knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = "!H")

reticulate::use_python("/Users/rmn/miniconda3/envs/dataScience/bin/python")

```



```{python pytest, echo=FALSE, include=FALSE}
from sys import version, executable
print(f"Python Version: {version} \nPath: {executable}")
```



# Einleitung

## Forschungsfrage

*Kann das Suchverhalten der Denoising Autoencoder Genetic Programming (DAE-GP) Metaheuristik durch den Einsatz einer Pre-Training Strategie optimiert werden?*


Welchen Effekt hat Pre-Training auf:

  1. das Generalisierungsverhalten von DAE-GP?
  2. die Qualität der gefundenen Programme (Fitness/Programmlänge)?
  3. die Populationsdiversität?
  4. das Laufzeitverhalten?
  
Anwendungsgebiet: Symbolische Regression, insbesondere am Aifoil Datensatz
  


# Denoising Autoencoder Genetic Programming 

## Übersicht

* Metaheuristik basierend auf genetischer Programmierung (GP)
* Ersetzung der Variationsoperatoren von GP durch künstliche, neuronalen Netzen zur Optimierung des Suchverhaltens^[@dae-gp_2020_rtree]
* Variante des Estimation of Distribution-GP (EDA-GP)
* Einsatz von Pre-Training in mehreren Publikationen als möglicher Weg für eine weitere Optimierung vorgeschlagen^[@dae-gp_2022_symreg] ^[@daegp_explore_exploit]


## Estimation of Distribution Algorithmen (EDA)

* Entwicklung neuer Rekombinationsoperatoren für evolutionäre Algorithmen basierend auf dem Einsatz von probabilistischen Modellen^[@design_of_modern_heuristics]
* Hypothese: Problemspezifische Abhängigkeiten zwischen Entscheidungsvariablen können bei der Erzeugung neuer Individuen besser berücksichtigt werden als bei traditionellen Rekombinationsoperatoren^[@edaOrig1996]
* Weitere Verbreitung im Bereich der genetischen Algorithmen (GA) als für GP

## Denoising Autoencoder Estimation of Distribution Algorithmen (DAE-EDA)

Idee: Einsatz von Denoising Autoencoders^[@dae_orig2008] (DAE) als probabilistisches Modell für genetische Algorithmen ^[@harmless_overfitting_eda]

2 Phasen Ansatz:

  1. Model Building: Modell "lernt" die Eigenschaften von ausgewählten Lösungen hoher Güte durch das Trainieren eines DAEs
  2. Model Sampling: Neue Lösungen werden erzeugt durch das propagieren von bestehenden, mutierten Lösungen durch das erlernte Modell 
  

## Denoising Autoencoder Genetic Programming (DAE-GP)

* Adaptierung des DAE-EDA Algorithmus auf GP
* Darstellung von Individuen als Zeichenketten in prefix Notation
* seq2seq learning Problem: Einsatz von DAE - Long Short Term Memory Netzwerken (DAE-LSTM)

## DAE-GP Ablauf

![Flowchart - Regular DAE-GP](./img/flowcharts/dae-gp.png){height=80%}

## Pre-Training

Idee: Modelle werden vor ihrem eigentlichen Einsatz zum Lösen eines Problems auf möglichst großen Datensätzen vortrainiert

Mögliche Vorteile durch Pre-Training^[@pmlr-v5-erhan09a]: 

1. Geringere Bedarf an Trainings Daten für vortrainierte Modelle
2. Reduktion der Trainingszeiten/Laufzeiten
3. Verbesserung der Güte des Modells
4. Verbessertes Generalisierungsverhalten des Modells


# Aktueller Forschungsstand

## Generalisiertes Royal Tree Problem (GRT)^[@dae-gp_2020_rtree]:

  * Einfaches Suchproblem mit hoher Lokalität
  * DAE-GP erzeugt durch Model Sampling Lösungskandidaten mit höherer Fitness als GP 
  * Hohe Güte der erzeugten Lösungskandidaten resultiert in besserer Performance
  * Perfomance Vorteil steigt mit zunehmender Komplexität des GRT Problems
  
  
## Symbolische Regression^[@dae-gp_2022_symreg]:

  * Airfoil Datensatz für Real-World symbolische Regression
  * DAE-GP erzeugt für eine vorgegebene Anzahl von Fitness Evaluationen im Vergleich zu GP:
  
    1. Lösungen mit höherer Fitness
    2. Lösungen  mit geringerer Größe


## Pre-Training für Denoising Autoencoders^[@pmlr-v5-erhan09a]

### Positive Wirkung von Pre-Training auf DAE

1. Gesteigerte Modell Performance (sinkender Testfehler)
2. Besserer Generalisierungsfähigkeit
3. Erhöhter Robustness des Algorithmus (sinkende Varianz des Testfehlers)


### Einfluss der Modell Architektur

* Positiver Effekt steigt mit zunehmender Komplexitität des DAE
* Je mehr versteckte Layer oder die Anzahl an Neuronen pro verstecktem Layer vorhanden sind, desto besserer Effekt des Pre-Trainings
* Für sehr kleine DAE, zeigt Pre-Training jedoch inverse, negative Auswirkung auf die Modell Performance



# Implementation

## Überblick

Gewählte Pre-Training Strategie: 

* (Klassisches) Pre-Training: Einmaliges Pre-Training eines DAE-LSTM mit einer großen Population aus Lösungskandidaten $\hat{P}_{train}$

* Trainingsmethode Early Stopping: Abbruch des Trainings sobald der Testfehler für eine seperate Population $\hat{P}_{test}$ konvergiert

*Ausschluss anderer Pre-Training Strategien wie Re-Use Learning, Few-Shot Learning*

## Pre-Trained DAE-GP Ablauf

![Flowchart - Pre-Trained DAE-GP](./img/flowcharts/pt-dae-gp.png){height=75%}

## Herausforderungen

... hidden neurons ... 


# Experiment Generalisierungsverhalten 

## Fragestellung

Welchen Einfluss hat die Dimension der eingesetzten DAE-LSTMs auf das Generalisierungsverhalten von DAE-GP in Kombination mit Pre-Training?

Experimenteller Aufbau: 

* Ausschließliche Betrachtung des Rekonstruktionsfehlers der ersten Generation (Nutzung von seperaten Test- und Trainingspopulationen)
* DAE-LSTM Training erfolgt über eine fixe Anzahl von $5000$ Epochen (kein frühzeitiger Abbruch bei Konvergenz des Testfehlers)

Insgesamt 12 Subexperimente mit jeweils 10 Durchläufen ($120$ Gesamtdurchläufe):

* Variable Anzahl von Hidden Neurons (50, 100, 200) mit einem Hidden Layer
* Variable Anzahl von Hidden Layers (1, 2, 3) mit 100 Hidden Neurons
* (normales) DAE-GP und pre-trained DAE-GP

## Einfluss der Anzahl von Hidden Layers

![Airfoil - Erste Generation Median Trainingsfehler - Variable Anzahl Hidden Layer](./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_layers.png){height=85%}

## Einfluss der Anzahl von Hidden Neurons

![Airfoil - Erste Generation Median Trainingsfehler - Variable Anzahl Hidden Neurons](./img/airfoil_firstGen/airfoil_firstGen_median_training_error_by_neurons.png){height=85%}

## Interpretation

...

# Experimente Suchverhalten Symbolische Regression

## Fragestellung

Welchen Einfluss hat die Verwendung einer Pre-Training Strategie auf das Suchverhalten von DAE-GP bei der Anwendung auf symbolische Regressionsprobleme?

Aufbau: Betrachtung des Suchverhalten über je 10 Gesamtduchläufe für DAE-GP, pre-trained DAE-GP und reguläres GP ($30$ Durchläufe gesamt)

Fokus insbesondere auf:

* Lösungsgüte
* Größe der gefundenen Lösungen (=Anzahl an Knoten)
* Populationsdiversität

## Hyperparameter

```{r airfoil_fullRun_2hl_maxIndSize_params, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"
knitr::kable(
  read.csv("./tables/airfoil_fullRun_2hl_maxIndSize_params"),
  # caption="Hyperparameter"
)%>% row_spec(0,bold=TRUE ) %>% kableExtra::kable_styling(latex_options = "hold_position")
```
## Funktions Set Symbolische Regression

```{r airfoil_function_set, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"
knitr::kable(
  read.csv("./tables/airfoil_function_set"),
  # caption="Funktions Set"
)%>% row_spec(0,bold=TRUE ) %>% kableExtra::kable_styling(latex_options = "hold_position")
```

## Lösungsgüte Airfoil

![Airfoil Datensatz - Fitness über Generationen](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/mean_median_fitness_byGens.png){height=90%}

## Verteilung Lösungsgüte Airfoil

![Airfoil Datensatz - Fitness über Generationen](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png){height=90%}

## Lösungsgröße Airfoil

![Airfoil Datensatz - Größe der besten Lösung über Generationen](~/masterThesis/master_thesis/img/airfoil_2hl_maxIndSize_fullRun_30gens_withGP/mean_Size_byGens.png)

## Kontrollexperiment: Reduzierung der DAE-LSTM Dimension Airfoil

Frage: Welchen Einfluss hat die Reduktion der DAE-LSTM auf 1 hidden Layer?

## Lösungsgüte Airfoil mit einem hidden Layer

![Airfoil Datensatz - 1 HL - Lösungsgüte über Generationen](~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/mean_median_fitness_byGens.png)

## Verteilung Lösungsgüte Airfoil mit einem hidden Layer

![Airfoil Datensatz - 1 HL - Lösungsgüte über Generationen](~/masterThesis/master_thesis/img/airfoil_1hl_maxIndSize_fullRun_30gens/final_fit_boxplot.png)

## Anwendung auf weiteren Datensätzen

Airfoil Datensatz: Ergebnis deuten bei einer ausreichenden Anzahl von Hidden Layern auf einen postitiven Effekt der Pre-Training Strategie hin:

* Höhere Fitness der gefundenen Lösungen
* Kleinere Größe der gefundenen Lösungen

Daher: Ausweitung des Experiments auf weitere Datensätze

## Übersicht Datensätze

```{r full_run_realWorldSymReg_problems, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"
knitr::kable(
  read.csv("./tables/full_run_realWorldSymReg_problems.csv"),
  #caption="Overview - Real World Symbolic Regression Benchmark Problems"
)%>% row_spec(0,bold=TRUE ) %>% kableExtra::kable_styling(latex_options = "hold_position")
```

## Auswertung Fitness - Symbolische Regression


```{r full_run_realWorldSymReg_fitness, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_final_fit_mean.csv",
  # "/Users/rmn/github/master_thesis/data/summary_table_final_fit_median.csv",  -> Use this path to summarize by median
  sep=",",
  colClasses = c("character", "numeric", "character", "numeric", "numeric", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],4)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],4))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],4)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],4))
  }
}
# shorten column names
col_names <- c("Problem", "Hid.Layers", "Set", "DAE-GP", "Pre-Trained", "P-Value")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% row_spec(0,bold=FALSE ) %>% kableExtra::kable_styling(latex_options = "hold_position", font_size = 7)

```
^[P-Werte markiert mit (*,**,***) für $\alpha$ ($0.1$, $0.05$ and $0.01$), Statistischer Test: Mann-Whitney U]

## Fitness - Energy Cooling Datensatz

![Energy (Cooling) Datensatz - Fitness über Generationen](~/masterThesis/master_thesis/img/energyCooling_2hl_FullRun_30gens/mean_median_fitness_byGens.png)

## Fitness - Boston Housing Datensatz

![Boston Housing Datensatz - Fitness über Generationen](~/masterThesis/master_thesis/img/bostonHousing_2hl_maxIndSize_fullRun_30gens/mean_median_fitness_byGens.png)

## Zusammenfassung Fitness - Symbolische Regression

Keine Evidenz für einen statistisch signifikanten Einfluss von Pre-Training auf die erzielte Lösungsqualität in den durchgeführten Experimenten!

* Airfoil:
* Energy Cooling:
* Boston Housing: 
* Concrete: 


## Auswertung Mean der Größe der besten Individuen - Symbolische Regression

```{r full_run_realWorldSymReg_best_solution_size, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
library(knitr)
library(kableExtra)
library(dplyr)

# Create the data frame from csv
df <- read.csv(
  "/Users/rmn/github/master_thesis/data/summary_table_best_size_mean.csv",
  sep=",",
  colClasses = c("character", "numeric", "numeric", "numeric", "character")
)

# Create a new dataframe with the modified values of the two columns,
df_bold <- df 

for(i in 1:nrow(df)){
  if(df[i, "DAE.GP"] < df[i, "Pre.Trained_DAE.GP"]){
    df_bold[i, "DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "DAE.GP"],4)))
    df_bold[i, "Pre.Trained_DAE.GP"] <- as.character(round(df[i, "Pre.Trained_DAE.GP"],4))
    }
  else if (df[i, "DAE.GP"] > df[i, "Pre.Trained_DAE.GP"]) {
    df_bold[i, "Pre.Trained_DAE.GP"] <- sub("^(.*)$", "**\\1**", as.character(round(df[i, "Pre.Trained_DAE.GP"],4)))
  df_bold[i, "DAE.GP"] <- as.character(round(df[i, "DAE.GP"],4))
  }
}
# shorten column names
col_names <- c("Problem", "Hid.Layers", "DAE-GP", "Pre-Trained", "P-Value")


knitr::kable(
 df_bold,
 format = "markdown",
 col.names = col_names,
 align = "c"
 #caption = "Symbolic Regression Overview - Mean final Fitness"
) %>% row_spec(0,bold=FALSE ) %>% kableExtra::kable_styling(latex_options = "hold_position", font_size = 7)

```




# Weitere Ansätze


# References {.allowframebreaks} 







