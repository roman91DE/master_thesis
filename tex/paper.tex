% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{float}
\usepackage[numbers]{natbib}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Pre-Trained Denoising Autoencoders Long Short-Term Memory
Networks as probabilistic Models for Estimation of Distribution Genetic
Programming

--------------------------------------------------------

Student: Roman HÃ¶hn

Date of Birth: 1991-04-14

Place of Birth: Wiesbaden, Hesse

Student ID: 2712497

Supervisor: David Wittenberg

--------------------------------------------------------

Master Thesis

FB 03: Chair of Business Administration and Computer Science

Johannes Gutenberg University Mainz}
\author{}
\date{\vspace{-2.5em}Date of Submission: 2022-09-11}

\begin{document}
\maketitle

\thispagestyle{empty} \newpage{}

\setcounter{page}{1}

\tableofcontents
\thispagestyle{empty}
\newpage

\hypertarget{overview}{%
\section{Overview}\label{overview}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Denoising Autoencoder Genetic Programming (DAE-GP) is a novel variation
of an genetic programming based Estimation of Distribution Algorithm
(EDA-GP) that uses a denoising autoencoders long short-term memory
network (DAE-LSTMs) as a probabilistic model to sample new candidate
solutions (Wittenberg, Rothlauf and Schweim, 2020).

DAE-LSTMs are recurrent neural networks (RNNs) that can be trained in an
unsupervised learning environment to minimize a reconstruction error for
encoding input data into a compressed representation and subsequently
decoding the compressed representation back to the input dimension. In
DAE-GP, DAE-LSTMs are trained with a subset of high-fitness solutions
selected from a parent population with the aim to capture their
promising qualities. The resulting model is then used to sample new
offspring solutions by propagating partially mutated solutions from the
parent population through the DAE-LSTMS (Wittenberg, Rothlauf and
Schweim, 2020).

In previous work DAE-GP has been shown to outperform GP for both a
generalized version of the royal tree problem (Wittenberg, Rothlauf and
Schweim, 2020) as well as for a real-world symbolic regression problem
(Wittenberg and Rothlauf, 2022). The DAE-GP algorithm that was used for
both experiments initializes the DAE-LSTMs with randomized parameters
for each generation \(g\) of the search. This thesis investigates a
possible way of improving the performance of DAE-GP by using pre-trained
DAE-LSTMs with parameters that are passed on from each generation
\(g_{t}\) to the next generation \(g_{t+1}\). Pre-Training is a commonly
used strategy in deep architectures that has been shown to improve both
the optimization process itself as well as the generalization behavior
if compared to randomly initialized parameters (Erhan \emph{et al.},
2009).

\hypertarget{research-question}{%
\subsection{Research Question}\label{research-question}}

This thesis studies the influence of using pre-trained DAE-LSTMs in
DAE-GP on the quality of solutions found. Recent research published by
Wittenberg, Rothlauf and Schweim (2020) and Wittenberg and Rothlauf
(2022) introduced DAE-GP as a promising alternative approach to existing
EDA-GP based metaheuristics. If it is possible to further improve
solution quality of DAE-GP by using pre-trained DAE-LSTMs this research
might contribute positively to future research and the practical
adoption of DAE-GP.

The experiments that are conducted follow a similar research design as
the initial publication describing DAE-GP by Wittenberg, Rothlauf and
Schweim (2020). A generalized version of the royal tree problem will be
used as a benchmark problem to compare the performance of standard
DAE-GP to a modified DAE-GP algorithm that uses pre-training for the
initialization of it's DAE-LSTMs. The royal tree problem is an
adjustable benchmark problem for GP that allows to examine differences
in optimization performance for varying degrees of problem complexity
(Kinnear \emph{et al.}, 1996). Both DAE-GP algorithms will be repeatedly
applied to three different configurations of the royal tree problem with
varying degrees of complexity. The results of the experiment will then
be used to test for statistical differences in the underlying
distributions with the aim to answer the question if pre-training will
significantly influence the quality of solutions found by DAE-GP.

In addition to comparing the average solution quality for both
approaches, this thesis will also focus on exploring differences in the
candidate solutions that are created by sampling pre-trained DAE-LSTMs
in comparison to randomly initialized DAE-LSTMs. Here, the main
questions of interest is, if pre-training can significantly improve the
fitness of offspring solutions that are sampled during DAE-GP if
compared to a DAE-GP algorithm that does not use pre-training.

\newpage

\hypertarget{I}{%
\section*{References}\label{I}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-pmlr-v5-erhan09a}{}}%
Erhan, D. \emph{et al.} (2009) {`The difficulty of training deep
architectures and the effect of unsupervised pre-training'}, in D. van
Dyk and M. Welling (eds) \emph{Proceedings of the twelth international
conference on artificial intelligence and statistics}. Hilton Clearwater
Beach Resort, Clearwater Beach, Florida USA: PMLR (Proceedings of
machine learning research), pp. 153--160. Available at:
\url{https://proceedings.mlr.press/v5/erhan09a.html}.

\leavevmode\vadjust pre{\hypertarget{ref-royal_tree_article}{}}%
Kinnear, K. \emph{et al.} (1996) {`The royal tree problem, a benchmark
for single and multi-population genetic programming'}.

\leavevmode\vadjust pre{\hypertarget{ref-dae-gp_2022_symreg}{}}%
Wittenberg, D. and Rothlauf, F. (2022) {`Denoising autoencoder genetic
programming for real-world symbolic regression'}, in \emph{Proceedings
of the genetic and evolutionary computation conference companion}. New
York, NY, USA: Association for Computing Machinery (GECCO '22), pp.
612--614. Available at: \url{https://doi.org/10.1145/3520304.3528921}.

\leavevmode\vadjust pre{\hypertarget{ref-dae-gp_2020_rtree}{}}%
Wittenberg, D., Rothlauf, F. and Schweim, D. (2020) {`DAE-GP: Denoising
autoencoder LSTM networks as probabilistic models in estimation of
distribution genetic programming'}, in \emph{Proceedings of the 2020
genetic and evolutionary computation conference}. New York, NY, USA:
Association for Computing Machinery (GECCO '20), pp. 1037--1045.
Available at: \url{https://doi.org/10.1145/3377930.3390180}.

\end{CSLReferences}

\end{document}
